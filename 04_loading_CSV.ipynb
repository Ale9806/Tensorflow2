{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Load CSV  in TensorFlow 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.2.0\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "# Make numpy values easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Data\n",
    "First we will get the data, for this we will use the `keras`API, `tf.keras.utils.get_file` method: This will download the dataset, only if the data set does not exist already in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LINKS TO  DOWNLOAD DATA SETS ###\n",
    "TRAIN_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\"\n",
    "TEST_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/eval.csv\"\n",
    "\n",
    "### CHECK IF Datasets DIR EXISTS, IF NOT IT CREATES IT ###\n",
    "if os.path.isdir('./Datasets') == False:\n",
    "    os.mkdir(\"Datasets\")\n",
    "\n",
    "### PATH TO STore DATA SETS ###\n",
    "path =  os.getcwd() + \"\\\\Datasets\"\n",
    "\n",
    "### GET DATA SETS, ONLY IF DATA SET IS NOT DOWNLOADED ALREADY  ####\n",
    "train_file_path = tf.keras.utils.get_file(path + \"\\\\train.csv\", TRAIN_DATA_URL)\n",
    "test_file_path = tf.keras.utils.get_file(path +\"\\\\eval.csv\", TEST_DATA_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data \n",
    "The only column you need to identify explicitly is the one with the value that the model is intended to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_COLUMN = 'survived'\n",
    "LABELS = [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now read the CSV data from the file and create a dataset.\n",
    "\n",
    "(For the full documentation, see [tf.data.experimental.make_csv_dataset](https://www.tensorflow.org/api_docs/python/tf/data/experimental/make_csv_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset shapes: (OrderedDict([(sex, (None,)), (age, (None,)), (n_siblings_spouses, (None,)), (parch, (None,)), (fare, (None,)), (class, (None,)), (deck, (None,)), (embark_town, (None,)), (alone, (None,))]), (None,)), types: (OrderedDict([(sex, tf.string), (age, tf.float32), (n_siblings_spouses, tf.int32), (parch, tf.int32), (fare, tf.float32), (class, tf.string), (deck, tf.string), (embark_town, tf.string), (alone, tf.string)]), tf.int32)> \n",
      "\n",
      "type: <class 'tensorflow.python.data.ops.dataset_ops.PrefetchDataset'>\n"
     ]
    }
   ],
   "source": [
    "def get_dataset(file_path, **kwargs):\n",
    "    \"\"\" Transform a CSV file to a Tf.data object:\n",
    "        INPUTS:\n",
    "        file_path: String: path to .csv file \n",
    "        **kargs:   Enables user add more paramerters inside:\n",
    "                   tf.data.experimental.make_csv_dataset()\n",
    "    \"\"\"\n",
    "    \n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        file_path, \n",
    "        batch_size=5, \n",
    "        label_name=LABEL_COLUMN, \n",
    "        na_value=\"?\",\n",
    "        num_epochs=1, \n",
    "        ignore_errors=True,  \n",
    "        **kwargs)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "raw_train_data = get_dataset(train_file_path)\n",
    "raw_test_data = get_dataset(test_file_path)\n",
    "\n",
    "### View the object ###\n",
    "print(raw_train_data,\"\\n\")\n",
    "print(f\"type: {type(raw_train_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each item in the dataset is a batch, represented as a tuple of (many examples, many labels). The data from the examples is organized in column-based tensors (rather than row-based tensors), each with as many elements as the batch size (5 in this case).\n",
    "\n",
    "It might help to see this yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex                 : [b'male' b'male' b'male' b'female' b'male']\n",
      "age                 : [49. 28. 22. 49. 25.]\n",
      "n_siblings_spouses  : [1 0 0 0 1]\n",
      "parch               : [0 0 0 0 0]\n",
      "fare                : [89.104  7.775  7.796 25.929 26.   ]\n",
      "class               : [b'First' b'Third' b'Third' b'First' b'Second']\n",
      "deck                : [b'C' b'unknown' b'unknown' b'D' b'unknown']\n",
      "embark_town         : [b'Cherbourg' b'Southampton' b'Southampton' b'Southampton' b'Southampton']\n",
      "alone               : [b'n' b'y' b'y' b'y' b'n']\n"
     ]
    }
   ],
   "source": [
    "def show_batch(dataset):\n",
    "    \"\"\" Show a batch of the Dataset\n",
    "    INPUTS:\n",
    "    dataset  a tf... object\n",
    "    \"\"\"\n",
    "    for batch, label in dataset.take(1):\n",
    "        for key, value in batch.items():\n",
    "            print(f\"{key:20s}: {value.numpy()}\")\n",
    "            \n",
    "show_batch(raw_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the columns in the CSV are named. The dataset constructor will pick these names up automatically. If the file you are working with does not contain the column names in the first line, pass them in a list of strings to the column_names argument in the make_csv_dataset function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex                 : [b'female' b'female' b'male' b'male' b'female']\n",
      "age                 : [42. 30. 35. 28. 18.]\n",
      "n_siblings_spouses  : [0 3 0 1 2]\n",
      "parch               : [0 0 0 2 2]\n",
      "fare                : [227.525  21.     10.5    23.45  262.375]\n",
      "class               : [b'First' b'Second' b'Second' b'Third' b'First']\n",
      "deck                : [b'unknown' b'unknown' b'unknown' b'unknown' b'B']\n",
      "embark_town         : [b'Cherbourg' b'Southampton' b'Southampton' b'Southampton' b'Cherbourg']\n",
      "alone               : [b'y' b'n' b'y' b'n' b'n']\n"
     ]
    }
   ],
   "source": [
    "### LIST OF NAMES FOR CSV COLUMNS ###\n",
    "CSV_COLUMNS = ['survived', 'sex', 'age', 'n_siblings_spouses', 'parch', 'fare', 'class', 'deck', 'embark_town', 'alone']\n",
    "\n",
    "### LOAD CSV TO TF ###\n",
    "temp_dataset = get_dataset(train_file_path, column_names=CSV_COLUMNS)\n",
    "\n",
    "### SHOW A BATCH OF 5 elements ###\n",
    "show_batch(temp_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is going to use all the available columns. If you need to omit some columns from the dataset, create a list of just the columns you plan to use, and pass it into the (optional) select_columns argument of the constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_siblings_spouses  : [0 0 0 0 0]\n",
      "class               : [b'Third' b'Third' b'Third' b'Second' b'Third']\n",
      "deck                : [b'unknown' b'unknown' b'unknown' b'unknown' b'unknown']\n",
      "alone               : [b'y' b'y' b'y' b'y' b'y']\n"
     ]
    }
   ],
   "source": [
    "SELECT_COLUMNS = ['survived', 'n_siblings_spouses', 'class', 'deck', 'alone']\n",
    "\n",
    "temp_dataset = get_dataset(train_file_path, select_columns=SELECT_COLUMNS)\n",
    "\n",
    "show_batch(temp_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "A CSV file can contain a variety of data types. Typically you want to convert from those mixed types to a fixed length vector before feeding the data into your model.\n",
    "\n",
    "TensorFlow has a built-in system for describing common input conversions: tf.feature_column, see this tutorial for details.\n",
    "\n",
    "You can preprocess your data using any tool you like (like nltk or sklearn), and just pass the processed output to TensorFlow.\n",
    "\n",
    "The primary advantage of doing the preprocessing inside your model is that when you export the model it includes the preprocessing. This way you can pass the raw data directly to your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Continuous data \n",
    "If your data is already in an appropriate numeric format, you can pack the data into a vector before passing it off to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age                 : [28. 25. 16. 17. 25.]\n",
      "n_siblings_spouses  : [0. 1. 0. 4. 0.]\n",
      "parch               : [0. 0. 0. 2. 0.]\n",
      "fare                : [7.75  7.925 7.75  7.925 7.896]\n"
     ]
    }
   ],
   "source": [
    "SELECT_COLUMNS = ['survived', 'age', 'n_siblings_spouses', 'parch', 'fare']\n",
    "DEFAULTS = [0, 0.0, 0.0, 0.0, 0.0]\n",
    "temp_dataset = get_dataset(train_file_path, select_columns=SELECT_COLUMNS,column_defaults = DEFAULTS)\n",
    "\n",
    "show_batch(temp_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_batch, labels_batch = next(iter(temp_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 16.      0.      0.      7.75 ]\n",
      " [ 27.      0.      2.     11.133]\n",
      " [ 50.      2.      0.    133.65 ]\n",
      " [ 28.      1.      0.     19.967]\n",
      " [ 28.      0.      0.    110.883]]\n",
      "\n",
      "[1 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "def pack(features, label):\n",
    "    return tf.stack(list(features.values()), axis=-1), label\n",
    "\n",
    "packed_dataset = temp_dataset.map(pack)\n",
    "\n",
    "for features, labels in packed_dataset.take(1):\n",
    "  print(features.numpy())\n",
    "  print()\n",
    "  print(labels.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have mixed datatypes you may want to separate out these simple-numeric fields. The tf.feature_column api can handle them, but this incurs some overhead and should be avoided unless really necessary. Switch back to the mixed dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex                 : [b'female' b'female' b'male' b'male' b'male']\n",
      "age                 : [30. 15. 25. 28. 16.]\n",
      "n_siblings_spouses  : [1 0 0 0 0]\n",
      "parch               : [1 1 0 0 0]\n",
      "fare                : [ 24.15  211.337  13.      7.896   9.5  ]\n",
      "class               : [b'Third' b'First' b'Second' b'Third' b'Third']\n",
      "deck                : [b'unknown' b'B' b'unknown' b'unknown' b'unknown']\n",
      "embark_town         : [b'Southampton' b'Southampton' b'Southampton' b'Southampton'\n",
      " b'Southampton']\n",
      "alone               : [b'n' b'n' b'y' b'y' b'y']\n"
     ]
    }
   ],
   "source": [
    "show_batch(raw_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_batch, labels_batch = next(iter(temp_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So define a more general preprocessor that selects a list of numeric features and packs them into a single column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PackNumericFeatures(object):\n",
    "  def __init__(self, names):\n",
    "    self.names = names\n",
    "\n",
    "  def __call__(self, features, labels):\n",
    "    numeric_features = [features.pop(name) for name in self.names]\n",
    "    numeric_features = [tf.cast(feat, tf.float32) for feat in numeric_features]\n",
    "    numeric_features = tf.stack(numeric_features, axis=-1)\n",
    "    features['numeric'] = numeric_features\n",
    "\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERIC_FEATURES = ['age','n_siblings_spouses','parch', 'fare']\n",
    "\n",
    "packed_train_data = raw_train_data.map(\n",
    "    PackNumericFeatures(NUMERIC_FEATURES))\n",
    "\n",
    "packed_test_data = raw_test_data.map(\n",
    "    PackNumericFeatures(NUMERIC_FEATURES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex                 : [b'male' b'male' b'female' b'male' b'male']\n",
      "class               : [b'Third' b'Third' b'Third' b'Third' b'Second']\n",
      "deck                : [b'unknown' b'unknown' b'unknown' b'unknown' b'unknown']\n",
      "embark_town         : [b'Southampton' b'Southampton' b'Southampton' b'Southampton'\n",
      " b'Southampton']\n",
      "alone               : [b'y' b'y' b'n' b'y' b'y']\n",
      "numeric             : [[28.     0.     0.     7.775]\n",
      " [28.     0.     0.    14.5  ]\n",
      " [18.     1.     0.    17.8  ]\n",
      " [28.     0.     0.     8.05 ]\n",
      " [24.     0.     0.    13.   ]]\n"
     ]
    }
   ],
   "source": [
    "show_batch(packed_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_batch, labels_batch = next(iter(packed_train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuous data should always be normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>n_siblings_spouses</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>627.000000</td>\n",
       "      <td>627.000000</td>\n",
       "      <td>627.000000</td>\n",
       "      <td>627.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>29.631308</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.379585</td>\n",
       "      <td>34.385399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12.511818</td>\n",
       "      <td>1.151090</td>\n",
       "      <td>0.792999</td>\n",
       "      <td>54.597730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.895800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.045800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>35.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.387500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              age  n_siblings_spouses       parch        fare\n",
       "count  627.000000          627.000000  627.000000  627.000000\n",
       "mean    29.631308            0.545455    0.379585   34.385399\n",
       "std     12.511818            1.151090    0.792999   54.597730\n",
       "min      0.750000            0.000000    0.000000    0.000000\n",
       "25%     23.000000            0.000000    0.000000    7.895800\n",
       "50%     28.000000            0.000000    0.000000   15.045800\n",
       "75%     35.000000            1.000000    0.000000   31.387500\n",
       "max     80.000000            8.000000    5.000000  512.329200"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc = pd.read_csv(train_file_path)[NUMERIC_FEATURES].describe()\n",
    "desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29.631  0.545  0.38  34.385] [12.512  1.151  0.793 54.598]\n"
     ]
    }
   ],
   "source": [
    "MEAN = np.array(desc.T['mean'])\n",
    "STD = np.array(desc.T['std'])\n",
    "print(MEAN, STD)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a numeric column. The tf.feature_columns.numeric_column API accepts a normalizer_fn argument, which will be run on each batch.\n",
    "\n",
    "Bind the MEAN and STD to the normalizer fn using functools.partial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NumericColumn(key='numeric', shape=(4,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function normalize_numeric_data at 0x000001F7F7C5B8B8>, mean=array([29.631,  0.545,  0.38 , 34.385]), std=array([12.512,  1.151,  0.793, 54.598])))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what you just created.\n",
    "def normalize_numeric_data(data, mean, std):\n",
    "    return (data-mean)/std # Center the data\n",
    "\n",
    "normalizer = functools.partial(normalize_numeric_data, mean=MEAN, std=STD)\n",
    "\n",
    "\n",
    "numeric_column = tf.feature_column.numeric_column('numeric', normalizer_fn=normalizer, shape=[len(NUMERIC_FEATURES)])\n",
    "numeric_columns = [numeric_column]\n",
    "numeric_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 4), dtype=float32, numpy=\n",
       "array([[ 27.   ,   0.   ,   0.   ,  76.729],\n",
       "       [ 17.   ,   0.   ,   2.   , 110.883],\n",
       "       [ 15.   ,   1.   ,   0.   ,  14.454],\n",
       "       [ 28.   ,   2.   ,   0.   ,  21.679],\n",
       "       [  0.75 ,   2.   ,   1.   ,  19.258]], dtype=float32)>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_batch['numeric']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.21 , -0.474, -0.479,  0.776],\n",
       "       [-1.01 , -0.474,  2.043,  1.401],\n",
       "       [-1.169,  0.395, -0.479, -0.365],\n",
       "       [-0.13 ,  1.264, -0.479, -0.233],\n",
       "       [-2.308,  1.264,  0.782, -0.277]], dtype=float32)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_layer = tf.keras.layers.DenseFeatures(numeric_columns)\n",
    "numeric_layer(example_batch).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Categorical data\n",
    "ome of the columns in the CSV data are categorical columns. That is, the content should be one of a limited set of options.\n",
    "\n",
    "Use the tf.feature_column API to create a collection with a tf.feature_column.indicator_column for each categorical column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='sex', vocabulary_list=('male', 'female'), dtype=tf.string, default_value=-1, num_oov_buckets=0)),\n",
       " IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='class', vocabulary_list=('First', 'Second', 'Third'), dtype=tf.string, default_value=-1, num_oov_buckets=0)),\n",
       " IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='deck', vocabulary_list=('A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'), dtype=tf.string, default_value=-1, num_oov_buckets=0)),\n",
       " IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='embark_town', vocabulary_list=('Cherbourg', 'Southhampton', 'Queenstown'), dtype=tf.string, default_value=-1, num_oov_buckets=0)),\n",
       " IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='alone', vocabulary_list=('y', 'n'), dtype=tf.string, default_value=-1, num_oov_buckets=0))]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "CATEGORIES = {\n",
    "    'sex': ['male', 'female'],\n",
    "    'class' : ['First', 'Second', 'Third'],\n",
    "    'deck' : ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'],\n",
    "    'embark_town' : ['Cherbourg', 'Southhampton', 'Queenstown'],\n",
    "    'alone' : ['y', 'n']\n",
    "}\n",
    "\n",
    "\n",
    "categorical_columns = []\n",
    "for feature, vocab in CATEGORIES.items():\n",
    "  cat_col = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "        key=feature, vocabulary_list=vocab)\n",
    "  categorical_columns.append(tf.feature_column.indicator_column(cat_col))\n",
    "    \n",
    "    \n",
    "# See what you just created.\n",
    "categorical_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A next step would be to build a build a tf.keras.Sequential, starting with the preprocessing_layer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning_env",
   "language": "python",
   "name": "machine_learning_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
