{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Load Numpy in TensorFlow 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.2.0\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "# Make numpy values easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Data\n",
    "First we will get the data, for this we will use the `keras`API, `tf.keras.utils.get_file` method: This will download the dataset, only if the data set does not exist already in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LINKS TO  DOWNLOAD DATA SETS ###\n",
    "DATA_URL = 'https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz'\n",
    "\n",
    "### CHECK IF Datasets DIR EXISTS, IF NOT IT CREATES IT ###\n",
    "if os.path.isdir('./Datasets') == False:\n",
    "    os.mkdir(\"Datasets\")\n",
    "\n",
    "### PATH TO STore DATA SETS ###\n",
    "path =  os.getcwd() + \"\\\\Datasets\"\n",
    "\n",
    "### GET DATA SETS, ONLY IF DATA SET IS NOT DOWNLOADED ALREADY  ####\n",
    "Data_set = tf.keras.utils.get_file(path + '\\\\mnist.npz', DATA_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "Load data from .npz file We use the MNIST dataset in Keras. Click here for more on .[npz files](https://numpy.org/doc/stable/reference/generated/numpy.savez.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GETTING TO KNOW THE DATA : \n",
      "\n",
      "TRAINING SET\n",
      "X_train: (60000, 28, 28), Type: <class 'numpy.ndarray'>\n",
      "y_train: (60000,),        Type: <class 'numpy.ndarray'>\n",
      "\n",
      "TESTING SET\n",
      "X_test: (10000, 28, 28), Type: <class 'numpy.ndarray'>\n",
      "y_test: (10000,),        Type: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "### USE A CONTEXT MANAGER TO LOAD DATA WITH NUMPY ###\n",
    "with np.load(Data_set) as data:\n",
    "    X_train = data['x_train']\n",
    "    y_train = data['y_train']\n",
    "    X_test = data['x_test']\n",
    "    y_test = data['y_test']\n",
    "    \n",
    "### GETTING TO KNOW THE DATASET ###\n",
    "space=\"      \" #define a string to add as space bar\n",
    "print(\"GETTING TO KNOW THE DATA : \\n\")\n",
    "print(\"TRAINING SET\")\n",
    "print(f\"X_train: {X_train.shape}, Type: {type(X_train)}\")\n",
    "print(f\"y_train: {y_train.shape}, {space} Type: {type( y_train)}\")\n",
    "print(\"\\nTESTING SET\")\n",
    "print(f\"X_test: {X_test.shape}, Type: {type(X_test)}\")\n",
    "print(f\"y_test: {y_test.shape},  {space}Type: {type(y_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load NumPy arrays with tf.data.Dataset\n",
    "\n",
    "Assuming you have an array of examples and a corresponding array of labels, pass the two arrays as a tuple into tf.data.Dataset.from_tensor_slices to create a tf.data.Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train,y_train))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A next step would be to build a build a tf.keras.Sequential, starting with the preprocessing_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning_env",
   "language": "python",
   "name": "machine_learning_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
